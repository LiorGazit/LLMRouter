{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWQF8tTX9hEF"
      },
      "source": [
        "# UX for LLMPop - Working Simultanously with Multiple LLMs, Locally, Safely, for Free\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/LiorGazit/llmpop/blob/main/notebooks/multi_llm_webapp.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> (pick a GPU Colab session for fastest computing)  \n",
        "\n",
        "This notebook is a minimal “click, pick, and prompt” UI for LLMPop that lets you select up to four models and compare their replies side by side.  \n",
        "It runs entirely free in your free Google Colab session and auto-handles local models via Ollama (no local installs on your machine).  \n",
        "Use the optional OpenAI API key only if you choose the gpt-4o tab, otherwise everything stays within your Colab runtime. \n",
        "It’s meant for quick demos and teaching, not production, so you can show friends how to use LLMs without touching code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ7BCyEi-njg"
      },
      "source": [
        "### Setting up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPw6vdPJc4qg",
        "outputId": "ec60de5a-db34-4c36-bd8b-824d7a640429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradio version: 4.44.1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Gradio Blocks instance: 13 backend functions\n",
              "--------------------------------------------\n",
              "fn_index=0\n",
              " inputs:\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489b35a30>\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489df9fa0>\n",
              "fn_index=1\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef48a00a9f0>\n",
              "fn_index=2\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba3a40>\n",
              "fn_index=3\n",
              " inputs:\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489bbd280>\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489ba3200>\n",
              "fn_index=4\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489ba2f30>\n",
              "fn_index=5\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba2ff0>\n",
              "fn_index=6\n",
              " inputs:\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489ba3020>\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489a28260>\n",
              "fn_index=7\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489a2a0f0>\n",
              "fn_index=8\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489a2a9c0>\n",
              "fn_index=9\n",
              " inputs:\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489ba2e40>\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489a2a750>\n",
              "fn_index=10\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef489a28080>\n",
              "fn_index=11\n",
              " inputs:\n",
              " outputs:\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489a2bcb0>\n",
              "fn_index=12\n",
              " inputs:\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba2e10>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba3a40>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba2ff0>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489a2a9c0>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489a2bcb0>\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489b35a30>\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489bbd280>\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489ba3020>\n",
              " |-<gradio.components.dropdown.Dropdown object at 0x7ef489ba2e40>\n",
              " |-<gradio.components.checkbox.Checkbox object at 0x7ef489b8aa80>\n",
              " |-<gradio.components.checkbox.Checkbox object at 0x7ef489bbc800>\n",
              " |-<gradio.components.checkbox.Checkbox object at 0x7ef489ba30b0>\n",
              " |-<gradio.components.checkbox.Checkbox object at 0x7ef489ba2e70>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489dfaae0>\n",
              " outputs:\n",
              " |-<gradio.components.html.HTML object at 0x7ef48a00a9f0>\n",
              " |-<gradio.components.html.HTML object at 0x7ef489ba2f30>\n",
              " |-<gradio.components.html.HTML object at 0x7ef489a2a0f0>\n",
              " |-<gradio.components.html.HTML object at 0x7ef489a28080>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba3a40>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489ba2ff0>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489a2a9c0>\n",
              " |-<gradio.components.textbox.Textbox object at 0x7ef489a2bcb0>\n",
              " |-<gradio.components.html.HTML object at 0x7ef489c04080>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =====================  ONE-CELL, SCHEMA-SAFE APP (HTML chats, string stores)  =====================\n",
        "import sys, subprocess\n",
        "subprocess.run(\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"--upgrade\", \"gradio==4.44.1\", \"llmpop\"],\n",
        "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True\n",
        ")\n",
        "\n",
        "import os, time, json, html\n",
        "os.environ[\"GRADIO_ANALYTICS_ENABLED\"] = \"false\"  # quieter logs\n",
        "\n",
        "import gradio as gr\n",
        "from llmpop import init_llm  # unified factory\n",
        "\n",
        "print(\"Gradio version:\", gr.__version__)\n",
        "\n",
        "# -------------------- Constants & mappings --------------------\n",
        "TAB_COLORS = [\"#000000\", \"#0A3D91\", \"#006400\", \"#FF1493\"]  # black, dark blue, dark green, pink\n",
        "\n",
        "CHOICE_LABELS = [\n",
        "    \"llama3.2:1b — Free & local (XXS)\",\n",
        "    \"codellama — Free & local (S)\",\n",
        "    \"deepseek-r1 — Free & local (S)\",\n",
        "    \"gpt-4o — OpenAI (API key)\",\n",
        "]\n",
        "LABEL_TO_ID = {\n",
        "    CHOICE_LABELS[0]: \"llama3.2:1b\",\n",
        "    CHOICE_LABELS[1]: \"codellama\",\n",
        "    CHOICE_LABELS[2]: \"deepseek-r1\",\n",
        "    CHOICE_LABELS[3]: \"gpt-4o\",\n",
        "}\n",
        "DEFAULT_LABELS = CHOICE_LABELS[:]\n",
        "\n",
        "def make_status(msg, color=None):\n",
        "    clr = color or \"#444\"\n",
        "    return f'<div style=\"font-family:system-ui,sans-serif;font-size:14px;color:{clr};\">{msg}</div>'\n",
        "\n",
        "def render_chat(history, slot_idx):\n",
        "    \"\"\"history = list of {'role','content'} dicts -> colored HTML.\"\"\"\n",
        "    color_user = \"#333\"\n",
        "    color_assistant = TAB_COLORS[slot_idx]\n",
        "    rows = []\n",
        "    for m in history:\n",
        "        role = m.get(\"role\", \"\")\n",
        "        content = html.escape(m.get(\"content\", \"\"))\n",
        "        if role == \"user\":\n",
        "            rows.append(f'<div style=\"margin:6px 0;padding:8px 10px;border-radius:10px;background:#f1f5f9;color:{color_user};\"><b>You:</b> {content}</div>')\n",
        "        else:\n",
        "            rows.append(f'<div style=\"margin:6px 0;padding:8px 10px;border-radius:10px;background:#fff0; border:1px solid {color_assistant};color:{color_assistant};\"><b>Model:</b> {content}</div>')\n",
        "    return \"<div>\" + \"\".join(rows) + \"</div>\"\n",
        "\n",
        "# -------------------- Model cache & helpers --------------------\n",
        "_model_cache = {}\n",
        "\n",
        "def get_model(model_name, provider, api_key, status_acc, color):\n",
        "    key = (model_name, provider, api_key or \"\")\n",
        "    if key in _model_cache:\n",
        "        return _model_cache[key]\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        if not api_key:\n",
        "            status_acc.append(make_status(f\"OpenAI model '{model_name}' selected but no API key provided.\", \"#B00020\"))\n",
        "            return None\n",
        "        status_acc.append(make_status(f\"Initializing OpenAI model: {model_name}…\", color))\n",
        "        mdl = init_llm(model=model_name, provider=\"openai\",\n",
        "                       provider_kwargs={\"api_key\": api_key}, temperature=0.0, verbose=False)\n",
        "        status_acc.append(make_status(f\"Ready: {model_name}\", color))\n",
        "    else:\n",
        "        status_acc.append(make_status(\n",
        "            f\"Starting/connecting to Ollama and pulling '{model_name}' (first time takes a bit)…\", color\n",
        "        ))\n",
        "        mdl = init_llm(model=model_name, provider=\"ollama\",\n",
        "                       provider_kwargs={\"pull\": True, \"auto_install\": True, \"auto_serve\": True},\n",
        "                       temperature=0.0, verbose=False)\n",
        "        status_acc.append(make_status(f\"Ready: {model_name}\", color))\n",
        "\n",
        "    _model_cache[key] = mdl\n",
        "    return mdl\n",
        "\n",
        "def append_turn(history, user, assistant):\n",
        "    history = history or []\n",
        "    history += [{\"role\": \"user\", \"content\": user},\n",
        "                {\"role\": \"assistant\", \"content\": assistant}]\n",
        "    return history\n",
        "\n",
        "# -------------------- UI --------------------\n",
        "with gr.Blocks(\n",
        "    title=\"LLMPop — Click · Pick · Prompt (Colab)\",\n",
        "    theme=gr.themes.Default(),\n",
        "    css=f\"\"\"\n",
        "    .llm-tab-0 .tabitem .tabs-label {{ color: {TAB_COLORS[0]} !important; }}\n",
        "    .llm-tab-1 .tabitem .tabs-label {{ color: {TAB_COLORS[1]} !important; }}\n",
        "    .llm-tab-2 .tabitem .tabs-label {{ color: {TAB_COLORS[2]} !important; }}\n",
        "    .llm-tab-3 .tabitem .tabs-label {{ color: {TAB_COLORS[3]} !important; }}\n",
        "    .slot-header-0 {{ color:{TAB_COLORS[0]}; font-weight:700; }}\n",
        "    .slot-header-1 {{ color:{TAB_COLORS[1]}; font-weight:700; }}\n",
        "    .slot-header-2 {{ color:{TAB_COLORS[2]}; font-weight:700; }}\n",
        "    .slot-header-3 {{ color:{TAB_COLORS[3]}; font-weight:700; }}\n",
        "    \"\"\"\n",
        ") as demo:\n",
        "    gr.Markdown(\"# Experiment with local LLMs in your private environment\\nPick LLMs on the left, tick which to invoke, then prompt below.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # ---- Left panel ----\n",
        "        with gr.Column(scale=1, min_width=320):\n",
        "            gr.Markdown(\"### LLM picks\")\n",
        "            model_dd, model_ck = [], []\n",
        "\n",
        "            for i, default_label in enumerate(DEFAULT_LABELS):\n",
        "                with gr.Row():\n",
        "                    dd = gr.Dropdown(\n",
        "                        choices=CHOICE_LABELS,\n",
        "                        value=default_label,\n",
        "                        label=f\"Slot {i+1} model\",\n",
        "                        allow_custom_value=False,\n",
        "                        filterable=False,\n",
        "                    )\n",
        "                    ck = gr.Checkbox(value=False, label=\"Invoke on next prompt\")\n",
        "                model_dd.append(dd)\n",
        "                model_ck.append(ck)\n",
        "\n",
        "            api_key = gr.Textbox(\n",
        "                label=\"OpenAI API Key (optional, for GPT-4o)\",\n",
        "                placeholder=\"sk-…\",\n",
        "                type=\"password\",\n",
        "                lines=1,\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"### Status\")\n",
        "            status_box = gr.HTML(make_status(\"Idle.\", \"#666\"))\n",
        "\n",
        "        # ---- Right panel ----\n",
        "        with gr.Column(scale=2, min_width=680):\n",
        "            tabs = gr.Tabs()\n",
        "            slot_headers = []\n",
        "            conv_html = []      # HTML render of chat per slot\n",
        "            hist_store = []     # hidden string stores (JSON list of messages)\n",
        "\n",
        "            for i in range(4):\n",
        "                with gr.Tab(f\"LLM Slot {i+1}\", elem_classes=[f\"llm-tab-{i}\"]):\n",
        "                    header = gr.HTML(f'<div class=\"slot-header-{i}\">Model: <b>{DEFAULT_LABELS[i]}</b></div>')\n",
        "                    html_box = gr.HTML(\"\")   # output-only chat render\n",
        "                    hidden_store = gr.Textbox(value=\"[]\", visible=False)  # stringified JSON\n",
        "\n",
        "                    slot_headers.append(header)\n",
        "                    conv_html.append(html_box)\n",
        "                    hist_store.append(hidden_store)\n",
        "\n",
        "            prompt = gr.Textbox(label=\"Prompt\", placeholder=\"Type your prompt here…\", lines=6)\n",
        "            send_btn = gr.Button(\"Send\")\n",
        "\n",
        "    # Header updates & tab reset (clear both chat render + store)\n",
        "    def header_html(label, idx):\n",
        "        return f'<div class=\"slot-header-{idx}\">Model: <b>{label}</b></div>'\n",
        "\n",
        "    for i, dd in enumerate(model_dd):\n",
        "        dd.change(fn=lambda label, idx=i: header_html(label, idx), inputs=dd, outputs=slot_headers[i])\n",
        "        dd.change(fn=lambda: \"\", inputs=None, outputs=conv_html[i])\n",
        "        dd.change(fn=lambda: \"[]\", inputs=None, outputs=hist_store[i])\n",
        "\n",
        "    # -------------------- Send (generator; histories via hidden Textbox strings) --------------------\n",
        "    def send(\n",
        "        p_text,\n",
        "        h1, h2, h3, h4,            # stringified JSON histories\n",
        "        l1, l2, l3, l4,            # dropdown labels\n",
        "        c1, c2, c3, c4,            # checkboxes\n",
        "        openai_key\n",
        "    ):\n",
        "        # parse stores -> lists\n",
        "        def parse_hist(s):\n",
        "            try:\n",
        "                v = json.loads(s or \"[]\")\n",
        "                return v if isinstance(v, list) else []\n",
        "            except Exception:\n",
        "                return []\n",
        "        histories = [parse_hist(h1), parse_hist(h2), parse_hist(h3), parse_hist(h4)]\n",
        "        labels    = [l1, l2, l3, l4]\n",
        "        checks    = [c1, c2, c3, c4]\n",
        "\n",
        "        def emit(status_html):\n",
        "            # outputs: conv_html(4), hist_store(4), status\n",
        "            return (\n",
        "                render_chat(histories[0], 0),\n",
        "                render_chat(histories[1], 1),\n",
        "                render_chat(histories[2], 2),\n",
        "                render_chat(histories[3], 3),\n",
        "                json.dumps(histories[0]),\n",
        "                json.dumps(histories[1]),\n",
        "                json.dumps(histories[2]),\n",
        "                json.dumps(histories[3]),\n",
        "                status_html\n",
        "            )\n",
        "\n",
        "        if not any(checks):\n",
        "            return emit(make_status(\"No LLM selected. Tick at least one checkbox to invoke.\", \"#B00020\"))\n",
        "        if not p_text or not p_text.strip():\n",
        "            return emit(make_status(\"Please enter a prompt.\", \"#B00020\"))\n",
        "\n",
        "        statuses = []\n",
        "        for idx in range(4):\n",
        "            if not checks[idx]:\n",
        "                continue\n",
        "\n",
        "            label = labels[idx]\n",
        "            model_id = LABEL_TO_ID.get(label)\n",
        "            if not model_id:\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] Invalid model selection.\", \"#B00020\"))\n",
        "                continue\n",
        "\n",
        "            provider = \"openai\" if model_id == \"gpt-4o\" else \"ollama\"\n",
        "            color = TAB_COLORS[idx]\n",
        "\n",
        "            if provider == \"openai\" and not (openai_key or \"\").strip():\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] '{model_id}' selected but no OpenAI API key provided.\", \"#B00020\"))\n",
        "                continue\n",
        "\n",
        "            statuses.append(make_status(f\"[Slot {idx+1}] Preparing {model_id}…\", color))\n",
        "\n",
        "            try:\n",
        "                mdl = get_model(model_id, provider, (openai_key or \"\").strip() or None, statuses, color)\n",
        "                if mdl is None:\n",
        "                    continue\n",
        "\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] Invoking {model_id}…\", color))\n",
        "\n",
        "                t0 = time.time()\n",
        "                out = mdl.invoke(p_text).content\n",
        "                dt = time.time() - t0\n",
        "\n",
        "                histories[idx] = append_turn(histories[idx], p_text, out)\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] {model_id} finished in {dt:.1f}s.\", color))\n",
        "\n",
        "            except Exception as e:\n",
        "                statuses.append(make_status(f\"[Slot {idx+1}] {model_id} error: {e!s}\", \"#B00020\"))\n",
        "\n",
        "        return emit(\"\".join(statuses) if statuses else make_status(\"Done.\", \"#666\"))\n",
        "\n",
        "    # Button wiring: inputs = prompt + *string stores* + dropdowns + checkboxes + key\n",
        "    send_btn.click(\n",
        "        fn=send,\n",
        "        inputs=[prompt, *hist_store, *model_dd, *model_ck, api_key],\n",
        "        outputs=[*conv_html, *hist_store, status_box],\n",
        "        concurrency_limit=1,\n",
        "    )\n",
        "\n",
        "# Queue + Launch\n",
        "demo.queue(max_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPVv9nA4-hOG"
      },
      "source": [
        "### Running the app:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "DAkXYTbA-cQb",
        "outputId": "d421f872-bf17-4d1f-a72c-8146fff27c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://96e7fbc4b04d37da75.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://96e7fbc4b04d37da75.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n",
            "All done setting up Ollama (ChatOllama).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "demo.launch(share=True, debug=True, max_threads=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P49ipVS-tPu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WWQF8tTX9hEF"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
